{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Teleoperation with Keyboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from mujoco_env.y_env import SimpleEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_path = './asset/example_scene_y.xml'\n",
    "PnPEnv = SimpleEnv(xml_path, seed = 0, state_type = 'joint_angle')\n",
    "action = np.zeros(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while PnPEnv.env.is_viewer_alive():\n",
    "    PnPEnv.step_env()\n",
    "    if PnPEnv.env.loop_every(HZ=20):\n",
    "\n",
    "        # Teleoperate robot\n",
    "        action, reset  = PnPEnv.teleop_robot()\n",
    "        if reset:\n",
    "            PnPEnv.reset(seed=0)\n",
    "\n",
    "        # Get the end-effector pose \n",
    "        ee_pose = PnPEnv.get_ee_pose()\n",
    "        # print(ee_pose)\n",
    "        agent_image,wrist_image = PnPEnv.grab_image()\n",
    "\n",
    "        # # resize to 256x256\n",
    "        agent_image = Image.fromarray(agent_image)\n",
    "        wrist_image = Image.fromarray(wrist_image)\n",
    "\n",
    "        agent_image = agent_image.resize((256, 256))\n",
    "        wrist_image = wrist_image.resize((256, 256))\n",
    "\n",
    "        agent_image = np.array(agent_image)\n",
    "        wrist_image = np.array(wrist_image)\n",
    "\n",
    "        joint_q = PnPEnv.step(action)\n",
    "\n",
    "        PnPEnv.render(teleop=True)\n",
    "\n",
    "PnPEnv.env.close_viewer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Odometry Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from mujoco_env.y_env import SimpleEnv\n",
    "\n",
    "xml_path = './asset/example_scene_y.xml'\n",
    "PnPEnv = SimpleEnv(xml_path, seed = 0, state_type = 'joint_angle')\n",
    "\n",
    "action = np.zeros(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_angle = 25 # degree, press -> key to triggger sim\n",
    "\n",
    "target_x = 0.05 #cm, press 'w-s' key to triggger sim\n",
    "target_y = 0.05 #cm, press 'a-d' key to triggger sim\n",
    "target_z = 0.05 #cm, press 'r-f' key to triggger sim\n",
    "\n",
    "\n",
    "while PnPEnv.env.is_viewer_alive():\n",
    "    PnPEnv.step_env()\n",
    "    if PnPEnv.env.loop_every(HZ=20):\n",
    "\n",
    "        # teleoperate the robot \n",
    "        action, reset  = PnPEnv.test_teleop_robot(w_yaw=target_angle, w_x=target_x, w_y=target_y, w_z=target_z)\n",
    "        if reset:\n",
    "            PnPEnv.reset(seed=0)\n",
    "\n",
    "        # get the end-effector pose \n",
    "        ee_pose = PnPEnv.get_ee_pose()\n",
    "        px, py, pz, roll, pitch, yaw = ee_pose\n",
    "        \n",
    "        # radian to degree\n",
    "        r_deg, p_deg, y_deg = np.rad2deg([roll, pitch, yaw])\n",
    "\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"ROBOT POSE (EE)\")\n",
    "        print(f\"Translation (m)  ->  X: {px:6.3f} | Y: {py:6.3f} | Z: {pz:6.3f}\")\n",
    "        print(f\"Orientation (deg) -> Roll: {int(r_deg):3d}° | Pitch: {int(p_deg):3d}° | Yaw: {int(y_deg):3d}°\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        agent_image,wrist_image = PnPEnv.grab_image()\n",
    "\n",
    "        # # resize to 256x256\n",
    "        agent_image = Image.fromarray(agent_image)\n",
    "        wrist_image = Image.fromarray(wrist_image)\n",
    "\n",
    "        agent_image = agent_image.resize((256, 256))\n",
    "        wrist_image = wrist_image.resize((256, 256))\n",
    "\n",
    "        agent_image = np.array(agent_image)\n",
    "        wrist_image = np.array(wrist_image)\n",
    "\n",
    "        joint_q = PnPEnv.step(action)\n",
    "\n",
    "        PnPEnv.render(teleop=True)\n",
    "\n",
    "PnPEnv.env.close_viewer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Visual Odometry with Hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Filter & Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter class \n",
    "class EMAFilter:\n",
    "    def __init__(self, alpha=0.3):\n",
    "        self.alpha = alpha\n",
    "        self.state = None\n",
    "\n",
    "    def apply(self, value):\n",
    "        if self.state is None:\n",
    "            self.state = value\n",
    "        else:\n",
    "            self.state = self.alpha * value + (1 - self.alpha) * self.state\n",
    "        return self.state\n",
    "\n",
    "# orientation filter\n",
    "filter_x = EMAFilter(alpha=0.2)\n",
    "filter_y = EMAFilter(alpha=0.2)\n",
    "filter_z = EMAFilter(alpha=0.2)\n",
    "\n",
    "# normal vector filter\n",
    "filter_norm = [EMAFilter(alpha=0.1) for _ in range(3)]\n",
    "\n",
    "# traslation filter\n",
    "filter_pos_x = EMAFilter(alpha=0.15)\n",
    "filter_pos_y = EMAFilter(alpha=0.15)\n",
    "filter_pos_z = EMAFilter(alpha=0.1) \n",
    "\n",
    "# homing\n",
    "reference_pos = None\n",
    "\n",
    "# mediapipe configs\n",
    "mp_hands = mp.tasks.vision.HandLandmarksConnections\n",
    "mp_drawing = mp.tasks.vision.drawing_utils\n",
    "mp_drawing_styles = mp.tasks.vision.drawing_styles\n",
    "\n",
    "MARGIN = 10  \n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 1  \n",
    "HANDEDNESS_TEXT_COLOR = (88, 205, 54)\n",
    "\n",
    "base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
    "options = vision.HandLandmarkerOptions(base_options=base_options, num_hands=2)\n",
    "detector = vision.HandLandmarker.create_from_options(options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Visualization Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# position\n",
    "last_stable_pos = [0.0, 0.0, 0.0] \n",
    "\n",
    "def draw_landmarks_on_image(rgb_image, detection_result, prev_angles=None):\n",
    "    global last_stable_pos\n",
    "    annotated_image = np.copy(rgb_image)\n",
    "    \n",
    "    height, width, _ = annotated_image.shape\n",
    "    gripper_state = False\n",
    "\n",
    "    camera_distance_m = 0.0\n",
    "    palm_normal = np.array([0.0, 0.0, -1.0])\n",
    "    current_angles = [0.0, 0.0, 0.0]\n",
    "    angle_deltas = [0.0, 0.0, 0.0]\n",
    "\n",
    "    final_x, final_y, final_z = last_stable_pos\n",
    "    \n",
    "    KNOWN_WIDTH_CM = 2.0\n",
    "    FOCAL_LENGTH = 700.0 \n",
    "\n",
    "    if not detection_result.hand_landmarks:\n",
    "        return annotated_image, gripper_state, tuple(last_stable_pos), palm_normal, current_angles\n",
    "\n",
    "    for idx, (hand_landmarks, world_landmarks) in enumerate(\n",
    "            zip(detection_result.hand_landmarks, detection_result.hand_world_landmarks)):\n",
    "\n",
    "        # depth Calculation \n",
    "        p0_px = (int(hand_landmarks[0].x * width), int(hand_landmarks[0].y * height))\n",
    "        p9_px = (int(hand_landmarks[9].x * width), int(hand_landmarks[9].y * height))\n",
    "        palm_center_px = ((p0_px[0] + p9_px[0]) // 2, (p0_px[1] + p9_px[1]) // 2)\n",
    "\n",
    "        p19_px = np.array([hand_landmarks[19].x * width, hand_landmarks[19].y * height])\n",
    "        p20_px = np.array([hand_landmarks[20].x * width, hand_landmarks[20].y * height])\n",
    "        pixel_dist = np.linalg.norm(p19_px - p20_px)\n",
    "        \n",
    "        if pixel_dist > 0:\n",
    "            camera_distance_m = (KNOWN_WIDTH_CM * FOCAL_LENGTH) / pixel_dist / 100.0\n",
    "        \n",
    "        raw_dist_cm = camera_distance_m * 100.0\n",
    "        raw_x_cm = ((palm_center_px[0] - width / 2) * raw_dist_cm) / FOCAL_LENGTH\n",
    "        raw_y_cm = ((palm_center_px[1] - height / 2) * raw_dist_cm) / FOCAL_LENGTH\n",
    "\n",
    "        # filtering\n",
    "        smooth_x = filter_pos_x.apply(raw_x_cm)\n",
    "        smooth_y = filter_pos_y.apply(raw_y_cm)\n",
    "        smooth_z = filter_pos_z.apply(raw_dist_cm)\n",
    "\n",
    "        # filtering\n",
    "        THRESHOLD = 0.4 \n",
    "        \n",
    "        final_x = smooth_x if abs(smooth_x - last_stable_pos[0]) > THRESHOLD else last_stable_pos[0]\n",
    "        final_y = smooth_y if abs(smooth_y - last_stable_pos[1]) > THRESHOLD else last_stable_pos[1]\n",
    "        final_z = smooth_z if abs(smooth_z - last_stable_pos[2]) > THRESHOLD else last_stable_pos[2]\n",
    "        \n",
    "        last_stable_pos = [final_x, final_y, final_z]\n",
    "\n",
    "        # palm normal\n",
    "        p0_w = np.array([world_landmarks[0].x, world_landmarks[0].y, world_landmarks[0].z])\n",
    "        p5_w = np.array([world_landmarks[5].x, world_landmarks[5].y, world_landmarks[5].z])\n",
    "        p17_w = np.array([world_landmarks[17].x, world_landmarks[17].y, world_landmarks[17].z])\n",
    "        \n",
    "        v1, v2 = p5_w - p0_w, p17_w - p0_w\n",
    "        raw_normal = np.cross(v2, v1)\n",
    "        if np.linalg.norm(raw_normal) > 0:\n",
    "            raw_normal /= np.linalg.norm(raw_normal)\n",
    "        \n",
    "        # palm normal filtering\n",
    "        palm_normal = np.array([\n",
    "            filter_norm[0].apply(raw_normal[0]),\n",
    "            filter_norm[1].apply(raw_normal[1]),\n",
    "            filter_norm[2].apply(raw_normal[2])\n",
    "        ])\n",
    "        palm_normal /= np.linalg.norm(palm_normal)\n",
    "\n",
    "        # degree filtering\n",
    "        raw_angles = [\n",
    "            np.degrees(np.arccos(np.clip(palm_normal[0], -1.0, 1.0))),\n",
    "            np.degrees(np.arccos(np.clip(palm_normal[1], -1.0, 1.0))),\n",
    "            np.degrees(np.arccos(np.clip(palm_normal[2], -1.0, 1.0)))\n",
    "        ]\n",
    "        current_angles = [\n",
    "            filter_x.apply(raw_angles[0]),\n",
    "            filter_y.apply(raw_angles[1]),\n",
    "            filter_z.apply(raw_angles[2])\n",
    "        ]\n",
    "\n",
    "        if prev_angles is not None:\n",
    "            angle_deltas = [current_angles[i] - prev_angles[i] for i in range(3)]\n",
    "\n",
    "        # gripper orientation\n",
    "        abs_vals = [abs(x) for x in palm_normal]\n",
    "        max_idx = abs_vals.index(max(abs_vals))\n",
    "        directions = [(\"Left\" if palm_normal[0]>0 else \"Right\"), \n",
    "                      (\"Down\" if palm_normal[1]>0 else \"Up\"), \n",
    "                      (\"In\" if palm_normal[2]>0 else \"Out\")]\n",
    "        dir_text = directions[max_idx]\n",
    "\n",
    "        p4_w = np.array([world_landmarks[4].x, world_landmarks[4].y, world_landmarks[4].z])\n",
    "        dist_gripper = np.linalg.norm(p4_w - np.array([world_landmarks[8].x, world_landmarks[8].y, world_landmarks[8].z])) * 100\n",
    "        \n",
    "        line_color = (0, 255, 0) \n",
    "        if dist_gripper < 8.0: \n",
    "            gripper_state = True\n",
    "            line_color = (255, 0, 0) \n",
    "\n",
    "\n",
    "        p4_px = (int(hand_landmarks[4].x * width), int(hand_landmarks[4].y * height))\n",
    "        p8_px = (int(hand_landmarks[8].x * width), int(hand_landmarks[8].y * height))        \n",
    "        cv2.line(annotated_image, p4_px, p8_px, line_color, 2)\n",
    "\n",
    "        # info panel\n",
    "        overlay = annotated_image.copy()\n",
    "        cv2.rectangle(overlay, (10, 10), (260, 480), (0, 0, 0), -1)\n",
    "        cv2.addWeighted(overlay, 0.7, annotated_image, 0.3, 0, annotated_image)\n",
    "\n",
    "        c_white, c_yellow, c_magenta = (255, 255, 255), (0, 255, 255), (255, 0, 255)\n",
    "        cv2.putText(annotated_image, f\"Orientation (degree) - {dir_text}\", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (150, 150, 150), 1)\n",
    "        \n",
    "        for i, ax in enumerate([\"X\", \"Y\", \"Z\"]):\n",
    "            y_pos = 80 + (i * 30)\n",
    "            d_color = (0, 0, 255) if angle_deltas[i] > 0.5 else (0, 255, 0) if angle_deltas[i] < -0.5 else c_white\n",
    "            cv2.putText(annotated_image, f\"{ax} Degree: {int(current_angles[i])} deg\", (20, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.5, c_yellow, 1)\n",
    "\n",
    "        cv2.putText(annotated_image, \"Translation (cm)\", (20, 220), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (150, 150, 150), 1)\n",
    "        cv2.putText(annotated_image, f\"X (Right-Left): {final_x:>5.1f} cm\", (20, 250), cv2.FONT_HERSHEY_SIMPLEX, 0.5, c_yellow, 1)\n",
    "        cv2.putText(annotated_image, f\"Y (Up-Down): {final_y:>5.1f} cm\", (20, 280), cv2.FONT_HERSHEY_SIMPLEX, 0.5, c_yellow, 1)\n",
    "        cv2.putText(annotated_image, f\"Z (Depth): {final_z:>5.1f} cm\", (20, 310), cv2.FONT_HERSHEY_SIMPLEX, 0.5, c_yellow, 1)\n",
    "\n",
    "        status_text = \"GRIPPER: Close\" if gripper_state else \"GRIPPER: Open\"\n",
    "        cv2.putText(annotated_image, status_text, (20, 450), cv2.FONT_HERSHEY_SIMPLEX, 0.8, line_color if gripper_state else line_color, 2)\n",
    "\n",
    "        cv2.circle(annotated_image, palm_center_px, 7, c_magenta, -1)\n",
    "        cv2.arrowedLine(annotated_image, palm_center_px, \n",
    "                        (int(palm_center_px[0] + palm_normal[0]*90), \n",
    "                        int(palm_center_px[1] + palm_normal[1]*90)), \n",
    "                c_white, 3)\n",
    "\n",
    "    mp_drawing.draw_landmarks(annotated_image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    return annotated_image, gripper_state, (final_x, final_y, final_z), palm_normal, current_angles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracted main information and data with this func:\n",
    "- Coordinates and orientation of hand center to move the ee\n",
    "- Distance between fingers joints to be able close and open the gripper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n",
    "    detection_result = detector.detect(mp_image)\n",
    "    \n",
    "    annotated_frame, state_, coords, palm_normal, current_angles = draw_landmarks_on_image(rgb_frame, detection_result)\n",
    "    \n",
    "    # print(\"[INFO] Palm XYZ: \", coords)\n",
    "    cv2.imshow('Hand Orientation', cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. System Integration Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is about end effector movement tests based on hand odometry which taken from webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from mujoco_env.y_env import SimpleEnv\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "import threading\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EMAFilter:\n",
    "    def __init__(self, alpha=0.3):\n",
    "        self.alpha = alpha\n",
    "        self.state = None\n",
    "    def apply(self, value):\n",
    "        if self.state is None: self.state = value\n",
    "        else: self.state = self.alpha * value + (1 - self.alpha) * self.state\n",
    "        return self.state\n",
    "\n",
    "filter_x = EMAFilter(alpha=0.2)\n",
    "filter_y = EMAFilter(alpha=0.2)\n",
    "filter_z = EMAFilter(alpha=0.2)\n",
    "filter_norm = [EMAFilter(alpha=0.1) for _ in range(3)]\n",
    "filter_pos_x = EMAFilter(alpha=0.15)\n",
    "filter_pos_y = EMAFilter(alpha=0.15)\n",
    "filter_pos_z = EMAFilter(alpha=0.1)\n",
    "\n",
    "ref_pos = None   # [x, y, z] cm\n",
    "ref_angles = None # [roll, pitch, yaw] deg\n",
    "last_stable_pos = [0.0, 0.0, 0.0]\n",
    "prev_palm_center = None\n",
    "alpha_p = 0.3  \n",
    "threshold_p = 10\n",
    "\n",
    "\n",
    "reference_pos = None\n",
    "\n",
    "mp_hands = mp.tasks.vision.HandLandmarksConnections\n",
    "mp_drawing = mp.tasks.vision.drawing_utils\n",
    "mp_drawing_styles = mp.tasks.vision.drawing_styles\n",
    "\n",
    "MARGIN = 10  \n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 1  \n",
    "HANDEDNESS_TEXT_COLOR = (88, 205, 54)\n",
    "\n",
    "base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
    "options = vision.HandLandmarkerOptions(base_options=base_options, num_hands=2)\n",
    "detector = vision.HandLandmarker.create_from_options(options)\n",
    "\n",
    "gripper_state_ = False\n",
    "final_rel_angles = [0.0, 0.0, 0.0]\n",
    "action = np.zeros(7)\n",
    "delta_ = [0, 0]\n",
    "camera_distance_m_ = 0.0\n",
    "coords_ = (0.0, 0.0, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBALS\n",
    "last_stable_pos = [0.0, 0.0, 0.0]\n",
    "ref_pos = None\n",
    "ref_angles = None\n",
    "\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "    global last_stable_pos, ref_pos, ref_angles\n",
    "\n",
    "    annotated_image = np.copy(rgb_image)\n",
    "    height, width, _ = annotated_image.shape\n",
    "\n",
    "    gripper_state = False\n",
    "    palm_normal = np.array([0.0, 0.0, -1.0])\n",
    "    current_angles = [0.0, 0.0, 0.0]\n",
    "\n",
    "    final_x, final_y, final_z = last_stable_pos\n",
    "    final_rel_pos = (0.0, 0.0, 0.0)\n",
    "    final_rel_angles = [0.0, 0.0, 0.0]\n",
    "\n",
    "    KNOWN_WIDTH_CM = 2.0\n",
    "    FOCAL_LENGTH = 700.0\n",
    "\n",
    "    if not detection_result.hand_landmarks:\n",
    "        ref_pos = None\n",
    "        ref_angles = None\n",
    "        return annotated_image, gripper_state, final_rel_angles, final_rel_pos\n",
    "\n",
    "    for hand_landmarks, world_landmarks in zip(\n",
    "        detection_result.hand_landmarks,\n",
    "        detection_result.hand_world_landmarks\n",
    "    ):\n",
    "\n",
    "        # 1. depth + position\n",
    "        p19_px = np.array([hand_landmarks[19].x * width, hand_landmarks[19].y * height])\n",
    "        p20_px = np.array([hand_landmarks[20].x * width, hand_landmarks[20].y * height])\n",
    "        pixel_dist = np.linalg.norm(p19_px - p20_px)\n",
    "\n",
    "        camera_distance_m = (KNOWN_WIDTH_CM * FOCAL_LENGTH) / pixel_dist / 100.0 if pixel_dist > 0 else 0.0\n",
    "        raw_dist_cm = camera_distance_m * 100.0\n",
    "\n",
    "        p0_px = (int(hand_landmarks[0].x * width), int(hand_landmarks[0].y * height))\n",
    "        p9_px = (int(hand_landmarks[9].x * width), int(hand_landmarks[9].y * height))\n",
    "        palm_center_px = ((p0_px[0] + p9_px[0]) // 2, (p0_px[1] + p9_px[1]) // 2)\n",
    "\n",
    "        # camera frame -cm\n",
    "        raw_x_cm = ((palm_center_px[0] - width / 2) * raw_dist_cm) / FOCAL_LENGTH\n",
    "        raw_y_cm = ((palm_center_px[1] - height / 2) * raw_dist_cm) / FOCAL_LENGTH\n",
    "\n",
    "        # 2. filtering\n",
    "        smooth_x = filter_pos_x.apply(raw_x_cm)\n",
    "        smooth_y = filter_pos_y.apply(raw_y_cm)\n",
    "        smooth_z = filter_pos_z.apply(raw_dist_cm)\n",
    "\n",
    "        THR = 0.4\n",
    "        final_x = smooth_x if abs(smooth_x - last_stable_pos[0]) > THR else last_stable_pos[0]\n",
    "        final_y = smooth_y if abs(smooth_y - last_stable_pos[1]) > THR else last_stable_pos[1]\n",
    "        final_z = smooth_z if abs(smooth_z - last_stable_pos[2]) > THR else last_stable_pos[2]\n",
    "\n",
    "        last_stable_pos = [final_x, final_y, final_z]\n",
    "\n",
    "\n",
    "        # 3. palm normal angles\n",
    "        p0_w = np.array([world_landmarks[0].x, world_landmarks[0].y, world_landmarks[0].z])\n",
    "        p5_w = np.array([world_landmarks[5].x, world_landmarks[5].y, world_landmarks[5].z])\n",
    "        p17_w = np.array([world_landmarks[17].x, world_landmarks[17].y, world_landmarks[17].z])\n",
    "\n",
    "        raw_normal = np.cross(p17_w - p0_w, p5_w - p0_w)\n",
    "        if np.linalg.norm(raw_normal) > 1e-6:\n",
    "            raw_normal /= np.linalg.norm(raw_normal)\n",
    "\n",
    "        palm_normal = np.array([filter_norm[i].apply(raw_normal[i]) for i in range(3)])\n",
    "        palm_normal /= np.linalg.norm(palm_normal)\n",
    "\n",
    "        raw_angles = [\n",
    "            np.degrees(np.arccos(np.clip(palm_normal[i], -1.0, 1.0)))\n",
    "            for i in range(3)\n",
    "        ]\n",
    "\n",
    "        current_angles = [\n",
    "            filter_x.apply(raw_angles[0]),\n",
    "            filter_y.apply(raw_angles[1]),\n",
    "            filter_z.apply(raw_angles[2])\n",
    "        ]\n",
    "\n",
    "        if ref_angles is None:\n",
    "            ref_angles = current_angles.copy()\n",
    "\n",
    "        final_rel_angles = [\n",
    "            current_angles[i] - ref_angles[i] for i in range(3)\n",
    "        ]\n",
    "\n",
    "        # 4. gripper\n",
    "        p4_w = np.array([world_landmarks[4].x, world_landmarks[4].y, world_landmarks[4].z])\n",
    "        p8_w = np.array([world_landmarks[8].x, world_landmarks[8].y, world_landmarks[8].z])\n",
    "        dist_gripper = np.linalg.norm(p4_w - p8_w) * 100\n",
    "\n",
    "        line_color = (0, 255, 0)\n",
    "        if dist_gripper < 6.0:\n",
    "            gripper_state = True\n",
    "            line_color = (0, 0, 255)\n",
    "\n",
    "        p4_px = (int(hand_landmarks[4].x * width), int(hand_landmarks[4].y * height))\n",
    "        p8_px = (int(hand_landmarks[8].x * width), int(hand_landmarks[8].y * height))\n",
    "        cv2.line(annotated_image, p4_px, p8_px, line_color, 2)\n",
    "\n",
    "        # 5. info panel\n",
    "        overlay = annotated_image.copy()\n",
    "        cv2.rectangle(overlay, (10, 10), (260, 480), (0, 0, 0), -1)\n",
    "        cv2.addWeighted(overlay, 0.7, annotated_image, 0.3, 0, annotated_image)\n",
    "\n",
    "        c_white = (255, 255, 255)\n",
    "        c_yellow = (0, 255, 255)\n",
    "        c_magenta = (255, 0, 255)\n",
    "\n",
    "        abs_vals = [abs(x) for x in palm_normal]\n",
    "        max_idx = abs_vals.index(max(abs_vals))\n",
    "        directions = [\n",
    "            \"Left\" if palm_normal[0] > 0 else \"Right\",\n",
    "            \"Down\" if palm_normal[1] > 0 else \"Up\",\n",
    "            \"In\" if palm_normal[2] > 0 else \"Out\"\n",
    "        ]\n",
    "        dir_text = directions[max_idx]\n",
    "\n",
    "        cv2.putText(\n",
    "            annotated_image,\n",
    "            f\"Orientation (degree) - {dir_text}\",\n",
    "            (20, 50),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.5,\n",
    "            (150, 150, 150),\n",
    "            1\n",
    "        )\n",
    "\n",
    "        for i, ax in enumerate([\"X\", \"Y\", \"Z\"]):\n",
    "            cv2.putText(\n",
    "                annotated_image,\n",
    "                f\"{ax} Degree: {int(final_rel_angles[i])} deg\",\n",
    "                (20, 80 + i * 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.5,\n",
    "                c_yellow,\n",
    "                1\n",
    "            )\n",
    "\n",
    "        cv2.putText(annotated_image, \"Translation (cm)\", (20, 220),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (150, 150, 150), 1)\n",
    "        cv2.putText(annotated_image, f\"X: {last_stable_pos[0]:5.1f}\", (20, 250),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, c_yellow, 1)\n",
    "        cv2.putText(annotated_image, f\"Y: {last_stable_pos[1]:5.1f}\", (20, 280),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, c_yellow, 1)\n",
    "        cv2.putText(annotated_image, f\"Z: {last_stable_pos[2]:5.1f}\", (20, 310),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, c_yellow, 1)\n",
    "\n",
    "        cv2.putText(\n",
    "            annotated_image,\n",
    "            \"GRIPPER: CLOSE\" if gripper_state else \"GRIPPER: OPEN\",\n",
    "            (20, 450),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.8,\n",
    "            line_color,\n",
    "            2\n",
    "        )\n",
    "\n",
    "        cv2.circle(annotated_image, palm_center_px, 7, c_magenta, -1)\n",
    "        cv2.arrowedLine(\n",
    "            annotated_image,\n",
    "            palm_center_px,\n",
    "            (int(palm_center_px[0] + palm_normal[0] * 90),\n",
    "             int(palm_center_px[1] + palm_normal[1] * 90)),\n",
    "            c_white,\n",
    "            3\n",
    "        )\n",
    "\n",
    "        mp_drawing.draw_landmarks(\n",
    "            annotated_image,\n",
    "            hand_landmarks,\n",
    "            mp_hands.HAND_CONNECTIONS\n",
    "        )\n",
    "\n",
    "    return annotated_image, gripper_state, final_rel_angles, last_stable_pos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Translation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def camera_thread():\n",
    "    global action, gripper_state_, final_rel_angles, coords_\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n",
    "        detection_result = detector.detect(mp_image)\n",
    "        \n",
    "        res = draw_landmarks_on_image(rgb_frame, detection_result)\n",
    "        annotated_frame, state_, final_rel_angles, coords_ = res\n",
    "        gripper_state_ = bool(state_)\n",
    "\n",
    "        cv2.imshow('Hand Orientation', cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR))\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "def sim_thread():\n",
    "    global action, coords_, final_rel_angles\n",
    "    \n",
    "    xml_path = './asset/example_scene_y.xml'\n",
    "    PnPEnv = SimpleEnv(xml_path, seed=0, state_type='joint_angle')\n",
    "    reset = False\n",
    "    \n",
    "    while PnPEnv.env.is_viewer_alive():\n",
    "        PnPEnv.step_env()\n",
    "        if PnPEnv.env.loop_every(HZ=50):\n",
    "            curr_ee_pose = PnPEnv.get_ee_pose()\n",
    "\n",
    "            current_gripper = gripper_state_\n",
    "            hand_x_cm, hand_y_cm, hand_z_cm = coords_\n",
    "            hand_y_cm = -hand_y_cm\n",
    "            \n",
    "            print(\"[INFO].. \",hand_x_cm, hand_y_cm, hand_z_cm)\n",
    "\n",
    "            if abs(hand_z_cm) > 70 or hand_z_cm == 0:\n",
    "                hand_z_cm_ = 0\n",
    "\n",
    "            if 10 < abs(hand_z_cm) < 60:\n",
    "                hand_z_cm_ = (60 - hand_z_cm) * 0.8\n",
    "\n",
    "            print(\"[INFO].. \",hand_x_cm, hand_y_cm, hand_z_cm)\n",
    "            \n",
    "            wrist_x = -final_rel_angles[0]\n",
    "            wrist_y = -final_rel_angles[1]\n",
    "            wrist_z = final_rel_angles[2]\n",
    "\n",
    "            # print(\"wrist x: \", wrist_x)\n",
    "            # print(\"wrist z: \", wrist_z)\n",
    "            action, reset  = PnPEnv.test_teleop_robot1(gripper_state=current_gripper, w_pitch = 0.0, w_roll = 0.0 , w_yaw= 0.0, \n",
    "                                                       w_x = 0.0, w_y = 0.0, w_z = hand_y_cm)\n",
    "\n",
    "\n",
    "            # action, reset  = PnPEnv.test_teleop_robot1(gripper_state=current_gripper, w_pitch = 0.0, w_roll = 0.0 , w_yaw= 0.0, w_x=0.0, w_y=0.0, w_z = hand_y_cm)\n",
    "            # action, reset  = PnPEnv.test_teleop_robot1(gripper_state=current_gripper, w_pitch = 0.0, w_roll = 0.0 , w_yaw= 0.0, w_x=hand_z_cm, w_y=0.0, w_z = 0.0)\n",
    "            # action, reset  = PnPEnv.test_teleop_robot1(gripper_state=current_gripper, w_pitch = 0.0, w_roll = 0.0 , w_yaw= 0.0, w_x=0.0, w_y=hand_x_cm, w_z = 0.0)\n",
    "\n",
    "\n",
    "            if reset:\n",
    "                PnPEnv.reset(seed=0)\n",
    "\n",
    "            # Get the end-effector pose and images\n",
    "            ee_pose = PnPEnv.get_ee_pose()\n",
    "\n",
    "            px, py, pz, roll, pitch, yaw = ee_pose\n",
    "            # print(f\"Pozisyon: X={px:.2f}, Y={py:.2f}, Z={pz:.2f}\")\n",
    "            print(f\"Rotasyon (Radyan): R={roll:.2f}, P={pitch:.2f}, Y={yaw:.2f}\")\n",
    "                        \n",
    "\n",
    "            agent_image,wrist_image = PnPEnv.grab_image()\n",
    "\n",
    "            # resize to 256x256\n",
    "            agent_image = Image.fromarray(agent_image)\n",
    "            wrist_image = Image.fromarray(wrist_image)\n",
    "\n",
    "            agent_image = agent_image.resize((256, 256))\n",
    "            wrist_image = wrist_image.resize((256, 256))\n",
    "\n",
    "            agent_image = np.array(agent_image)\n",
    "            wrist_image = np.array(wrist_image)\n",
    "\n",
    "            joint_q = PnPEnv.step(action)\n",
    "\n",
    "            PnPEnv.render(teleop=True)\n",
    "\n",
    "    PnPEnv.env.close_viewer()\n",
    "\n",
    "# Start threads\n",
    "t1 = threading.Thread(target=camera_thread)\n",
    "t2 = threading.Thread(target=sim_thread)\n",
    "t1.start()\n",
    "t2.start()\n",
    "t1.join()\n",
    "t2.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Orientation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def camera_thread():\n",
    "    global action, gripper_state_, final_rel_angles, coords_\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n",
    "        detection_result = detector.detect(mp_image)\n",
    "        \n",
    "        res = draw_landmarks_on_image(rgb_frame, detection_result)\n",
    "        annotated_frame, state_, final_rel_angles, coords_ = res\n",
    "        gripper_state_ = bool(state_)\n",
    "\n",
    "        cv2.imshow('Hand Orientation', cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR))\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "def sim_thread():\n",
    "    global action, coords_, final_rel_angles\n",
    "    \n",
    "    xml_path = './asset/example_scene_y.xml'\n",
    "    PnPEnv = SimpleEnv(xml_path, seed=0, state_type='joint_angle')\n",
    "    reset = False\n",
    "    \n",
    "    while PnPEnv.env.is_viewer_alive():\n",
    "        PnPEnv.step_env()\n",
    "        if PnPEnv.env.loop_every(HZ=50):\n",
    "            curr_ee_pose = PnPEnv.get_ee_pose()\n",
    "\n",
    "            current_gripper = gripper_state_\n",
    "            hand_x_cm, hand_y_cm, hand_z_cm = coords_\n",
    "            # print(\"[INFO].. \",hand_x_cm, hand_y_cm, hand_z_cm)\n",
    "\n",
    "            current_delta = delta_ if delta_ is not None else [0, 0]\n",
    "            current_distance = (camera_distance_m_ if camera_distance_m_ is not None else 0.0)*1000\n",
    "\n",
    "            target_pitch = np.rad2deg(PnPEnv.home_pitch) + final_rel_angles[0]\n",
    "            target_roll  = np.rad2deg(PnPEnv.home_roll)  + final_rel_angles[1]\n",
    "            target_yaw   = np.rad2deg(PnPEnv.home_yaw)   + final_rel_angles[2]\n",
    "\n",
    "            if hand_z_cm > 60 or hand_z_cm == 0:\n",
    "                hand_z_cm = 0\n",
    "            if 10 <= hand_z_cm <= 60:\n",
    "                hand_z_cm = (60 - hand_z_cm) * 1.2\n",
    "\n",
    "\n",
    "            \n",
    "            wrist_x = -final_rel_angles[0]\n",
    "            wrist_y = -final_rel_angles[1]\n",
    "            wrist_z = final_rel_angles[2]\n",
    "\n",
    "            # print(\"wrist x: \", wrist_x)\n",
    "            # print(\"wrist z: \", wrist_z)\n",
    "            action, reset  = PnPEnv.test_teleop_robot1(gripper_state=current_gripper, \n",
    "                                                       w_pitch = 0.0, w_roll = wrist_y , w_yaw= 0.0, \n",
    "                                                       w_x=0.0, w_y=0.0, w_z=0.0)\n",
    "\n",
    "            # action, reset  = PnPEnv.teleop_robot()\n",
    "\n",
    "\n",
    "            if reset:\n",
    "                PnPEnv.reset(seed=0)\n",
    "\n",
    "            # Get the end-effector pose and images\n",
    "            ee_pose = PnPEnv.get_ee_pose()\n",
    "\n",
    "            # İlk 3 eleman pozisyon (x, y, z), son 3 eleman rotasyon (roll, pitch, yaw)\n",
    "            px, py, pz, roll, pitch, yaw = ee_pose\n",
    "            # print(f\"Pozisyon: X={px:.2f}, Y={py:.2f}, Z={pz:.2f}\")\n",
    "            print(f\"Rotasyon (Radyan): R={roll:.2f}, P={pitch:.2f}, Y={yaw:.2f}\")\n",
    "                        \n",
    "\n",
    "            agent_image,wrist_image = PnPEnv.grab_image()\n",
    "\n",
    "            # resize to 256x256\n",
    "            agent_image = Image.fromarray(agent_image)\n",
    "            wrist_image = Image.fromarray(wrist_image)\n",
    "\n",
    "            agent_image = agent_image.resize((256, 256))\n",
    "            wrist_image = wrist_image.resize((256, 256))\n",
    "\n",
    "            agent_image = np.array(agent_image)\n",
    "            wrist_image = np.array(wrist_image)\n",
    "\n",
    "            joint_q = PnPEnv.step(action)\n",
    "\n",
    "            PnPEnv.render(teleop=True)\n",
    "\n",
    "    PnPEnv.env.close_viewer()\n",
    "\n",
    "# Start threads\n",
    "t1 = threading.Thread(target=camera_thread)\n",
    "t2 = threading.Thread(target=sim_thread)\n",
    "t1.start()\n",
    "t2.start()\n",
    "t1.join()\n",
    "t2.join()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lerobot_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
